[project]
name = "msc-ai-group-project"
version = "0.1.0"
description = "Exposing LLM uncertainty, unfairness, and other related trustworthiness metrics to users at response time."
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "gradio>=6.3.0",
    "huggingface-hub>=1.3.2,<2",
    "pyrefly>=0.47.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# =============================================
# PIXI CONFIGURATION - All dependency management
# =============================================

[tool.pixi.workspace]
channels = ["conda-forge"]
platforms = ["linux-64", "osx-64", "osx-arm64"]

# ---------------------------------------------
# Dependencies (conda-forge)
# ---------------------------------------------
[tool.pixi.dependencies]
python = "3.12.*"  # 3.12 required for llama-cpp-python CUDA wheels (no 3.13 builds yet)
pytest = ">=9.0"
ruff = ">=0.8"

# ---------------------------------------------
# CUDA Feature (for GPU users, Linux only)
# Using target.linux-64 specifiers since CUDA is not available on macOS.
# See: https://pixi.prefix.dev/dev/workspace/multi_platform_configuration/
# ---------------------------------------------
[tool.pixi.feature.cuda.target.linux-64.dependencies]
cuda-cudart = ">=12.4,<13"
cuda-nvrtc = ">=12.4,<13"
libcublas = ">=12.4,<13"

# Use pre-built CUDA wheels from the maintainer's index (not available on PyPI)
# See: https://github.com/abetlen/llama-cpp-python#supported-backends
[tool.pixi.feature.cuda.pypi-options]
extra-index-urls = ["https://abetlen.github.io/llama-cpp-python/whl/cu124"]

[tool.pixi.feature.cuda.target.linux-64.pypi-dependencies]
llama-cpp-python = ">=0.3,<1"
# TODO: Consider pinning torch to a specific CUDA version (e.g., via extra index URL)
# if GPU detection issues arise. Using "*" lets pixi resolve but may not guarantee CUDA backend.
torch = "*"
transformers = "*"
tqdm = ">=4.0"

# NOTE: LD_LIBRARY_PATH is required for llama-cpp-python to find the conda-installed
# CUDA libraries at runtime. Without this, it may fall back to CPU or fail to load.
# $CONDA_PREFIX is set automatically by pixi during environment activation.
[tool.pixi.feature.cuda.target.linux-64.activation]
env = { LD_LIBRARY_PATH = "$CONDA_PREFIX/lib:$LD_LIBRARY_PATH" }

[tool.pixi.feature.cuda.tasks]
download-models = "python scripts/download_model.py"
qwen = "PYTHONPATH=. python examples/qwen.py"
nli = "PYTHONPATH=. python examples/nli.py"
kle = "PYTHONPATH=. python examples/kle.py"

# ---------------------------------------------
# Environments
# ---------------------------------------------
[tool.pixi.environments]
default = { solve-group = "default" }
cuda = { features = ["cuda"], solve-group = "cuda" }

# ---------------------------------------------
# Tasks
# ---------------------------------------------
[tool.pixi.tasks]
# Linting
lint = "ruff check ."
lint-fix = "ruff check . --fix"

# Formatting
format = "ruff format ."
format-check = "ruff format . --check"

# Testing
test = "pytest"
test-cov = "pytest --cov"

# Type checking (strict - requires CUDA deps)
typecheck = "pyrefly check ."
# Lenient typecheck for CI (stubs CUDA deps as Any)
typecheck-lenient = "pyrefly check . --config pyrefly-lenient.toml"

# Application
serve = "python interface.py"

# CI (runs all checks - uses lenient typecheck since CUDA deps not available)
check = { depends-on = ["lint", "format-check", "typecheck-lenient", "test"] }

[tool.pyrefly]
search-path = ["."]
project-includes = [
    "**/*.py*",
    "**/*.ipynb",
]
